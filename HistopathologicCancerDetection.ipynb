{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import random\n",
    "from numpy import load\n",
    "from random import shuffle\n",
    "from math import *\n",
    "import os\n",
    "\n",
    "#Visualization :\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Scikit-learn utils, metrics :\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, plot_roc_curve\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "#Sklearn classification :\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Sklearn clustering :\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#Dimension reduction :\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "#Working with images :\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "#Model interpretability\n",
    "#!pip install captum\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import GradientShap\n",
    "from captum.attr import Occlusion\n",
    "from captum.attr import NoiseTunnel\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "#Pytorch specific :\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchvision import transforms, utils\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "# Notebook specific :\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analyzing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing data :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Path to local files :\n",
    "path_root = 'data'\n",
    "path_df = 'data/train_labels.csv'\n",
    "path_to_train = 'data/train/'\n",
    "path_to_test = 'data/test/'\n",
    "path_submission = 'data/sample_submission.csv'\n",
    "\n",
    "\n",
    "df_submission = pd.read_csv(path_submission)\n",
    "df = pd.read_csv(path_df)\n",
    "df_class0 = df.loc[df['label']==0]\n",
    "df_class1 = df.loc[df['label']==1]\n",
    "id_label_map = {k:v for k,v in zip(df.id.values, df.label.values)}\n",
    "dic_classes2 = {0:'No Cancer', 1:'Cancer'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"**Visualization of some random images :**\"\"\"\n",
    "\n",
    "## Visualizing class 0 :\n",
    "# Images with NO Tumor tissue in the center region (32x32 pixels)\n",
    "print(\"Class 0 - Contains NO tumor tissue\")\n",
    "plt.figure(figsize=(7,7))\n",
    "for i in range(16):\n",
    "    image = Image.open(os.path.join(path_to_train, df_class0['id'].sample().item()+'.tif'))\n",
    "    plt.subplot(4,4,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "## Visualizing class 0 :\n",
    "# Images with at least ONE pixel of Tumor tissue in the center region (32x32 pixels)\n",
    "print(\"\\n\",\"Class 1 - Contains at least ONE tumor tissue\")\n",
    "plt.figure(figsize=(7,7))\n",
    "for i in range(16):\n",
    "    image = Image.open(os.path.join(path_to_train, df_class1['id'].sample().item()+'.tif'))\n",
    "    plt.subplot(4,4,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some details about a random image :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(os.path.join(path_to_train, df_class1['id'].sample().item()+'.tif'))\n",
    "\n",
    "print(\"Image format:\",image.format)\n",
    "print(\"Image mode:\",image.mode)\n",
    "print('Image size:',image.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Labels main statistics :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cancer_detection_Dataset(Dataset):\n",
    "    \"\"\"Histopathologic-cancer-detection Dataset.\"\"\"\n",
    "    def __init__(self, root_dir, train_file, transform):\n",
    "        self.root_dir = root_dir\n",
    "        self.train_file_path = train_file\n",
    "        self.df_train = pd.read_csv(self.train_file_path)\n",
    "        self.transform = transform\n",
    "        self.classes = list(self.df_train['label'].unique())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return (image, target) after resize and preprocessing.\"\"\"\n",
    "        img = os.path.join(self.root_dir, self.df_train.iloc[idx, 0]+'.tif')\n",
    "        \n",
    "        X = Image.open(img)\n",
    "        y = self.class_to_index(self.df_train.iloc[idx, 1])\n",
    "        X = np.array(X)\n",
    "        X = torch.tensor(X)\n",
    "        X = X.float()\n",
    "        X = X.permute(2, 0, 1)\n",
    "\n",
    "        if self.transform:\n",
    "            X = self.transform(X)\n",
    "            #print(X.shape) #: 96x96x3\n",
    "        \n",
    "        y = torch.tensor([y])\n",
    "        return X, y\n",
    "    \n",
    "    def class_to_index(self, class_name):\n",
    "        \"\"\"Returns the index of a given class.\"\"\"\n",
    "        return self.classes.index(class_name)\n",
    "    \n",
    "    def index_to_class(self, class_index):\n",
    "        \"\"\"Returns the class of a given index.\"\"\"\n",
    "        return self.classes[class_index]\n",
    "    \n",
    "    def class_to_label(self, class_name):\n",
    "        \"\"\"Returns the label of a given class\"\"\"\n",
    "        return self.dic_classes[class_name]\n",
    "    \n",
    "    def get_class_count(self):\n",
    "        \"\"\"Return a list of label occurences\"\"\"\n",
    "        cls_count = dict(self.df_train.label.value_counts())\n",
    "        return cls_count\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the length of the dataset.\"\"\"\n",
    "        return len(self.df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = torch.nn.Sequential(\n",
    "    transforms.CenterCrop(90),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.Normalize((179.13/255, 139.29/255, 177.59/255), (46.11/255, 51.17/255, 41.27/255)))\n",
    "\n",
    "transform_test = torch.nn.Sequential(\n",
    "    transforms.CenterCrop(90),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.Normalize((179.13/255, 139.29/255, 177.59/255), (46.11/255, 51.17/255, 41.27/255)))\n",
    "\n",
    "dataset_train = Cancer_detection_Dataset(root_dir=path_to_train, train_file=path_df, transform=transform_train)\n",
    "dataset_test = Cancer_detection_Dataset(root_dir=path_to_test, train_file=path_submission, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_Eval_Model():\n",
    "    \"\"\"Train and / or evaluate a choosen model.\"\"\"\n",
    "    def __init__(self, net, dataset_train, dataset_test, batch_size):\n",
    "        self.dataset_train = dataset_train\n",
    "        self.dataset_test = dataset_test\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.validation_split = 0.1\n",
    "        self.shuffle_dataset = True\n",
    "        self.random_seed= 42\n",
    "        \n",
    "        # Average Mean and Std for all images in the training dataset, for all 3 layers\n",
    "        self.mean1 = 179.13/255\n",
    "        self.mean2 = 139.29/255\n",
    "        self.mean3 = 177.59/255\n",
    "        self.std1 = 46.11/255\n",
    "        self.std2 = 51.17/255\n",
    "        self.std3 = 41.27/255\n",
    "        \n",
    "        # Creating data indices for training and validation splits:\n",
    "        dataset_size = len(self.dataset_train)\n",
    "        indices = list(range(dataset_size))\n",
    "        split = int(np.floor(self.validation_split * dataset_size))\n",
    "\n",
    "        if self.shuffle_dataset :\n",
    "            np.random.seed(self.random_seed)\n",
    "            np.random.shuffle(indices)\n",
    "        train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "        # Creating data samplers and loaders:\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "        self.train_loader = torch.utils.data.DataLoader(self.dataset_train, self.batch_size, \n",
    "                                                   sampler=train_sampler)\n",
    "        self.validation_loader = torch.utils.data.DataLoader(self.dataset_train, self.batch_size,\n",
    "                                                        sampler=valid_sampler)\n",
    "        self.test_loader = torch.utils.data.DataLoader(self.dataset_test, self.batch_size)\n",
    "        \n",
    "        #Loading the model with cuda or cpu\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.net = net.to(self.device)\n",
    "        print(\"GPU :\",torch.cuda.get_device_name(0), \"activated\")\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Init OK\")\n",
    "        \n",
    "   \n",
    "    def denormalize(self,tensor):\n",
    "        \"\"\"Denormalize tensor to display true image\"\"\"\n",
    "        tensor[0,:,:] = tensor[0,:,:]*self.std1 + self.mean1\n",
    "        tensor[1,:,:] = tensor[1,:,:]*self.std2 + self.mean2\n",
    "        tensor[2,:,:] = tensor[2,:,:]*self.std3 + self.mean3\n",
    "        return tensor\n",
    "    \n",
    "    def auc_score(self, y_pred,y_true):\n",
    "        y_pred = y_pred.cpu().detach()\n",
    "        y_true = y_true.cpu().detach()\n",
    "        score=roc_auc_score(y_true,y_pred)\n",
    "        return score\n",
    "    \n",
    "    \n",
    "    def train(self, epochs:int, lr:float):\n",
    "        torch.cuda.empty_cache()\n",
    "        self.net.to(self.device)\n",
    "        \n",
    "        #Criterion and optimizer :\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = optim.Adam(self.net.parameters(), lr=lr)\n",
    "        \n",
    "        #Training :\n",
    "        self.net.train()\n",
    "        liste_loss = []\n",
    "        auc_train_mean, auc_val_mean = [], []\n",
    "        \n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            running_loss = 0.0\n",
    "            i = 0\n",
    "            for data_train, data_val in tqdm(zip(self.train_loader, self.validation_loader)):\n",
    "                # For the training set :\n",
    "                inputs_train, labels_train = data_train\n",
    "                labels_train = labels_train.to(torch.float32)\n",
    "                inputs_train, labels_train = inputs_train.to(self.device), labels_train.to(self.device)\n",
    "\n",
    "                #For the validation set :\n",
    "                inputs_val, labels_val = data_val\n",
    "                labels_val = labels_val.to(torch.float32)\n",
    "                inputs_val, labels_val = inputs_val.to(self.device), labels_val.to(self.device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs_train = self.net(inputs_train)\n",
    "                outputs_train = outputs_train.float()\n",
    "                outputs_train = outputs_train #Torch.round is ok too\n",
    "                outputs_train.to(self.device)\n",
    "\n",
    "                loss = criterion(outputs_train, labels_train)\n",
    "                loss.to(self.device)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                if i % 20 == 19:    # print every 20 batches\n",
    "                    with torch.no_grad():\n",
    "                        outputs_val = self.net(inputs_val)\n",
    "                        outputs_val = outputs_val.float()\n",
    "                        outputs_val = outputs_val\n",
    "\n",
    "                    auc_train = self.auc_score(outputs_train,labels_train)\n",
    "                    auc_val = self.auc_score(outputs_val,labels_val)\n",
    "                    auc_val_mean.append(auc_val)\n",
    "                    auc_train_mean.append(auc_train)\n",
    "\n",
    "                    print('[%d, %5d] loss: %.3f , AUC train: %.2f , AUC val: %.2f' %\n",
    "                          (epoch + 1, i + 1, running_loss, auc_train, auc_val))\n",
    "                    liste_loss.append(running_loss)\n",
    "                    running_loss = 0.0  \n",
    "                i+=1\n",
    "        print('Finished Training')\n",
    "        print(\"\\n\", \"AUC train mean:\", np.mean(auc_train_mean[-int(len(self.train_loader)/20):]))\n",
    "        print(\"\\n\", \"AUC val mean:\", np.mean(auc_val_mean[-int(len(self.validation_loader)/20):]))\n",
    "        plt.figure(figsize=(10,7))\n",
    "        plt.plot(liste_loss, label='Training loss')\n",
    "        plt.legend()\n",
    "        plt.title(\"Training loss\")\n",
    "        plt.show()\n",
    "\n",
    "        #Plot AUC evolution for Train and Val :\n",
    "        plt.figure(figsize=(10,7))\n",
    "        plt.plot(auc_train_mean,label='AUC train')\n",
    "        plt.plot(auc_val_mean,label='AUC val')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def save(self, out_name:str):\n",
    "        torch.save(self.net.state_dict(), out_name)\n",
    "        print('Model weights successfully saved !')\n",
    "        \n",
    "    def load(self, path_load:str):\n",
    "        self.net.load_state_dict(torch.load(path_load))\n",
    "        self.net.to(self.device)\n",
    "        print(\"Model loaded successfully\")\n",
    "        \n",
    "    def show_eval_mosaique(self):\n",
    "        \"\"\" Show a mosaique of 32 images, display\n",
    "        their true labels and the associated prediction\"\"\"\n",
    "        self.net.eval()\n",
    "        liste_auc = []\n",
    "        for i, data in enumerate(self.train_loader):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels  = data\n",
    "                labels = labels.to(torch.float32)\n",
    "                inputs = inputs.to(self.device)\n",
    "\n",
    "                outputs = self.net(inputs)\n",
    "                outputs = torch.round(outputs.float())\n",
    "                liste_auc.append(self.auc_score(outputs.cpu(),labels))\n",
    "            break\n",
    "\n",
    "        #Displaying the images and their true & predicted labels :\n",
    "        plt.figure(figsize=(25,50))\n",
    "        for j, image in enumerate(inputs):\n",
    "            image = image.cpu()\n",
    "            image = self.denormalize(image)\n",
    "            image = np.transpose(image, (1,2,0))\n",
    "            image = np.array(image, dtype=np.uint8)\n",
    "            plt.subplot(16,4,j+1)\n",
    "            plt.imshow(image)\n",
    "            plt.title(f'{dic_classes2[int(labels[j].item())]}, prediction {labels[j].item()==outputs[j].item()}',\n",
    "                     size=13)\n",
    "            plt.axis('off')\n",
    "        print(\"AUC mean on this batch :\", np.mean(liste_auc))\n",
    "        plt.show()\n",
    "        \n",
    "        print('\\n', \"Confusion Matrix:\")\n",
    "        conf_matrix = confusion_matrix(labels.detach().cpu().numpy(), outputs.detach().cpu().numpy(), labels=[0, 1])\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=[0, 1])\n",
    "        disp.plot()\n",
    "\n",
    "    \n",
    "    def explain_image(self,image):   \n",
    "        occlusion = Occlusion(self.net)\n",
    "\n",
    "        attributions_occ = occlusion.attribute(image.unsqueeze(0),\n",
    "                                           strides = (3, 4, 4),\n",
    "                                           target=None,\n",
    "                                           sliding_window_shapes=(3,5, 5),\n",
    "                                           baselines=0)\n",
    "\n",
    "        _ = viz.visualize_image_attr_multiple(np.transpose(attributions_occ.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                                          np.uint8(np.transpose(self.denormalize(image.squeeze().detach().cpu()), (1,2,0))),\n",
    "                                          [\"original_image\", \"heat_map\"],\n",
    "                                          [\"all\", \"positive\"],\n",
    "                                          show_colorbar=True,\n",
    "                                          outlier_perc=4,\n",
    "                                         cmap='Reds')\n",
    "        \n",
    "    def show_eval_explained(self, explain:bool):\n",
    "        \"\"\" Display a random image in the train set \n",
    "        and its heatmap with the occlusion method\"\"\"\n",
    "        self.net.eval()\n",
    "\n",
    "        rand = random.randint(0,len(self.train_loader))\n",
    "        inputs, labels = self.train_loader.dataset[rand][0], self.train_loader.dataset[rand][1]\n",
    "        labels = labels.to(torch.float32)\n",
    "        inputs_explain = inputs.clone().to(self.device)\n",
    "        inputs = inputs.unsqueeze(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.net(inputs)\n",
    "            print(\"Output probability: %.3f\" % (outputs.item()))\n",
    "            outputs = torch.round(outputs.float())\n",
    "\n",
    "        #Plotting the results :\n",
    "        image = inputs.squeeze().detach().cpu()\n",
    "        image = self.denormalize(image)\n",
    "\n",
    "        image = np.transpose(image, (1,2,0))\n",
    "        image = np.array(image, dtype=np.uint8)\n",
    "\n",
    "        plt.imshow(image)\n",
    "        plt.title(f'{dic_classes2[int(labels.item())]}, prediction {labels.item()==outputs.item()}', size=15)\n",
    "        plt.axis('Off')\n",
    "        plt.show()\n",
    "\n",
    "        if explain :\n",
    "            print(\"Interpreting prediction results. This could take up to 1 long minute...\")\n",
    "            print(\"Highlighting areas that led to that prediction...\")\n",
    "            self.explain_image(inputs_explain)\n",
    "            \n",
    "    def submit_test(self, filename:str):\n",
    "        self.net.eval()\n",
    "        liste_outputs = []\n",
    "        print('Evaluating test set...')\n",
    "        for i, data in tqdm(enumerate(self.test_loader)):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels  = data\n",
    "                labels = labels.to(torch.float32)\n",
    "                inputs = inputs.to(self.device)\n",
    "                outputs = self.net(inputs)\n",
    "                outputs = torch.round(outputs.float())\n",
    "                liste_outputs.append(outputs)\n",
    "        liste_outputs2 = []\n",
    "        for i in tqdm(range(len(liste_outputs))):\n",
    "            for j in liste_outputs[i]:\n",
    "                liste_outputs2.append(int(j.item()))\n",
    "        df_submission['label'] = liste_outputs2\n",
    "        df_submission.to_csv(filename, index=False)\n",
    "        print(\"Submission successfully downloaded !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining the Neural Network :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Net1(nn.Module):\n",
    "    def __init__(self, pretrained):\n",
    "        super(Net1, self).__init__()\n",
    "        self.model = models.densenet169(pretrained=pretrained)\n",
    "        self.linear1 = nn.Linear(1000+2, 512)\n",
    "        self.linear2 = nn.Linear(512, 16)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.elu = nn.ELU()\n",
    "        self.out = nn.Linear(16, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        batch = out.shape[0]\n",
    "        max_pool, _ = torch.max(out, 1, keepdim=True)\n",
    "        avg_pool = torch.mean(out, 1, keepdim=True)\n",
    "\n",
    "        out = out.view(batch, -1)\n",
    "        conc = torch.cat((out, max_pool, avg_pool), 1)\n",
    "\n",
    "        conc = self.linear1(conc)\n",
    "        conc = self.elu(conc)\n",
    "        conc = self.dropout(conc)\n",
    "        conc = self.linear2(conc)\n",
    "        conc = self.elu(conc)\n",
    "\n",
    "        res = self.out(conc)\n",
    "        return torch.sigmoid(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Train_Eval_Model(Net1(pretrained=True), dataset_train, dataset_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.train(epochs=5, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights successfully saved !\n"
     ]
    }
   ],
   "source": [
    "model.save(out_name='V11_best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.load('V11_best.pth')\n",
    "model.show_eval_mosaique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and explain the prediction :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, I use the library Captum that estimates which areas of the image are critical for the classifier's decision by occluding them and quantifying how the decision changes.\n",
    "\n",
    "In other words, for images containing a metastase, this script will highlight the part of the image that contains the cancer cell and that led to that prediction !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 1 :\n",
    "model.show_eval_explained(explain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 2 :\n",
    "model.show_eval_explained(explain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_Eval_Model(Train_Eval_Model):\n",
    "    # subclassing for continuity\n",
    "    def __init__(self, net, dataset_train, dataset_test, batch_size):\n",
    "        super(Train_Eval_Model, self).__init__(net, dataset_train, dataset_test, batch_size)\n",
    "        self.pca = PCA(n_components=64)\n",
    "        self.lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "        self.feature_extractor = models.densenet169(pretrained=True).to(self.device)\n",
    "        self.train_loader_extract = torch.utils.data.DataLoader(self.dataset_train, self.batch_size)\n",
    "    \n",
    "    def forward_feature_extract(self, x):\n",
    "        out = self.feature_extractor(x)\n",
    "        batch = out.shape[0]\n",
    "        max_pool, _ = torch.max(out, 1, keepdim=True)\n",
    "        avg_pool = torch.mean(out, 1, keepdim=True)\n",
    "        out = out.view(batch, -1)\n",
    "        x = torch.cat((out, max_pool, avg_pool), 1)\n",
    "        return(x)\n",
    "\n",
    "    \n",
    "    def extract_features(self, load_saved:bool):\n",
    "        \"\"\"\"Feature extraction + PCA to reduce dimension\"\"\"\n",
    "        \n",
    "        if load_saved :\n",
    "            #Load local npz files (to save time):\n",
    "            dict_X = load('X_features.npz')\n",
    "            dict_y = load('y_features.npz')\n",
    "\n",
    "            X_features = dict_X['arr_0']\n",
    "            y_features = dict_y['arr_0']\n",
    "            return(X_features, y_features)\n",
    "        \n",
    "        else :\n",
    "            self.net.eval()\n",
    "            liste_X_out, liste_y_out = [], []\n",
    "            for i, data in tqdm(enumerate(self.train_loader_extract)):\n",
    "                with torch.no_grad():\n",
    "                    inputs, labels  = data\n",
    "                    labels = labels.to(torch.float32)\n",
    "                    inputs = inputs.to(self.device)\n",
    "\n",
    "                    X = self.forward_feature_extract(inputs)\n",
    "                    X = X.detach().cpu().numpy()\n",
    "                    y = labels.detach().cpu().numpy()\n",
    "\n",
    "                    liste_X_out.append(X)\n",
    "                    liste_y_out.append(y)\n",
    "\n",
    "            return(liste_X_out, liste_y_out)\n",
    "    \n",
    "    def reduce_dimension(self, X, y, method:str):\n",
    "        if method == 'PCA':\n",
    "            self.reduce = self.pca.fit(X)\n",
    "            X = self.reduce.transform(X)\n",
    "        elif method == 'LDA':\n",
    "            self.reduce = self.lda.fit(X, y.ravel())\n",
    "            X = self.reduce.transform(X)\n",
    "        return(X,y)\n",
    "            \n",
    "    \n",
    "    def fit(self, X, y, model_ML:str, method_reduce:str):\n",
    "        \n",
    "        X, y = self.reduce_dimension(X, y, method=method_reduce)\n",
    "        print(\"Extracted features dimensions reduced to :\")\n",
    "        print(X.shape)\n",
    "        \n",
    "        if model_ML == 'adaboost':\n",
    "            self.clf = AdaBoostClassifier(n_estimators=100).fit(X, y.ravel())\n",
    "        elif model_ML == 'histgboost':\n",
    "            self.clf = HistGradientBoostingClassifier(learning_rate=0.1, max_iter=2000).fit(X, y.ravel())\n",
    "        elif model_ML == 'SVC':\n",
    "            self.clf = SVC(gamma='scale').fit(X, y)\n",
    "        elif model_ML == 'KNN':\n",
    "            self.clf = KNeighborsClassifier(n_neighbors=5).fit(X,y.ravel())\n",
    "        \n",
    "    \n",
    "    def predict_hybrid(self, loader_name:str):\n",
    "        if loader_name == 'train':\n",
    "            loader = self.train_loader\n",
    "        elif loader_name == 'val':\n",
    "            loader = self.validation_loader\n",
    "        elif loader_name == 'test':\n",
    "            loader = self.test_loader\n",
    "        elif loader_name == 'extract':\n",
    "            loader = self.train_loader_extract\n",
    "        \n",
    "        for i, data in enumerate(loader):\n",
    "                with torch.no_grad():\n",
    "                    inputs, labels  = data\n",
    "                    labels = labels.to(torch.float32)\n",
    "                    inputs = inputs.to(self.device)\n",
    "                    \n",
    "                    #Feature extraction :\n",
    "                    X = self.forward_feature_extract(inputs)\n",
    "                    X = X.detach().cpu().numpy()\n",
    "                    y_true = labels.detach().cpu().numpy()\n",
    "                    \n",
    "                    #Dimension reduction :\n",
    "                    X = self.reduce.transform(X)\n",
    "                    \n",
    "                    #Machine Learning prediction :\n",
    "                    #y_predict = self.clf.predict(np.tanh(X))\n",
    "                    y_predict = self.clf.predict(X)\n",
    "                print(\"AUC mean predicted on the batch :\",roc_auc_score(y_predict,y_true))\n",
    "                    \n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Train_Eval_Model(Net1(pretrained=True), dataset_train, dataset_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features, y_features = model.extract_features(load_saved=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_features, y_features, model_ML = 'histgboost', method_reduce='LDA')\n",
    "# model.fit(X_features, y_features, model_ML = 'histgboost', method_reduce:'PCA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_hybrid(loader_name='extract')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
